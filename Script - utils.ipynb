{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script - utils\n",
    "#### utils\n",
    "- model\n",
    "    - train()\n",
    "    - evaluate()\n",
    "    - generate()\n",
    "    - EarlyStopping\n",
    "- metrics\n",
    "    - bleu_score()\n",
    "    - unigram_f1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_target(ids, len_, padding_idx=3):\n",
    "    \n",
    "    # ids: sos + sequence + eos\n",
    "    len_ = len_-1\n",
    "    idx = torch.arange(len(len_), dtype=torch.long)\n",
    "    \n",
    "    # input_ids: sos + sequence\n",
    "    input_ids = ids.clone()\n",
    "    input_ids[idx, len_] = padding_idx\n",
    "    input_ids = input_ids[:,:-1]\n",
    "    input_ = input_ids\n",
    "    \n",
    "    # output_ids: sequence + eos\n",
    "    output_ids = ids.clone()\n",
    "    output_ids = output_ids[:,1:]\n",
    "    output = output_ids\n",
    "    \n",
    "    return input_, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batches(model, dataloader, criterion, optimizer, device):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "        \n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        id_, src_ids, src_len, tar_ids, tar_len  = batch\n",
    "        # src_ids shape: (batch_size, fixed_seq_len)\n",
    "        # src_len shape: (batch_size)\n",
    "        # tar_ids shape: (batch_size, fixed_seq_len)\n",
    "        # tar_len shape: (batch_size)\n",
    "        \n",
    "        src_input = (src_ids, src_len)\n",
    "        tar_input, tar_output = slice_target(tar_ids, tar_len)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        logits = model(src_input, tar_input)\n",
    "        \n",
    "        loss = criterion(logits, tar_output)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(epochs, model, dataloader, criterion, optimizer, device, early_stopping=None):\n",
    "    epochs = 15\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train_batches(model, train_dataloader, criterion, optimizer, device)\n",
    "        valid_loss = evaluate_batches(model, valid_dataloader, criterion, device)\n",
    "        print(f'Epoch {epoch+1:02d}| Train Loss : {train_loss}, Val. Loss : {valid_loss}')\n",
    "\n",
    "        if early_stopping is not None:\n",
    "            stop = early_stopping(valid_loss, model)\n",
    "            if stop:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batches(model, dataloader, criterion, device):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            \n",
    "            id_, src_ids, src_len, tar_ids, tar_len  = batch\n",
    "            # src_ids shape: (batch_size, fixed_seq_len)\n",
    "            # src_len shape: (batch_size)\n",
    "            # tar_ids shape: (batch_size, fixed_seq_len)\n",
    "            # tar_len shape: (batch_size)\n",
    "\n",
    "            src_input = (src_ids, src_len)\n",
    "            tar_input, tar_output = slice_target(tar_ids, tar_len)\n",
    "            \n",
    "            logits = model(src_input, tar_input)\n",
    "            # logits shape: (batch_size, tar_vocab_size, var_seq_len)\n",
    "            \n",
    "            batch_size, tar_vocab_size, pred_seq_len = logits.shape\n",
    "            true_seq_len = tar_output.shape[1]\n",
    "            \n",
    "            if pred_seq_len > true_seq_len:\n",
    "                pad = torch.ones((batch_size, pred_seq_len-true_seq_len), \n",
    "                                 dtype=torch.long, device=logits.device) * 3 # pad_id: 3\n",
    "                tar_output = torch.cat([tar_output, pad],1)\n",
    "                \n",
    "            elif pred_seq_len < true_seq_len:\n",
    "                tar_output = tar_output[:,:pred_seq_len]\n",
    "            \n",
    "            loss = criterion(logits, tar_output)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### generate(수정 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(model, dataloader, src_tokenizer, tar_tokenizer, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    Instance = namedtuple('Instance', ['id','src_sent', 'tar_sent'])\n",
    "    outputs = list()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            \n",
    "            id_, src_ids, src_len, tar_ids, tar_len  = batch\n",
    "            # src_ids shape: (batch_size, fixed_seq_len)\n",
    "            # src_len shape: (batch_size)\n",
    "            # tar_ids shape: (batch_size, fixed_seq_len)\n",
    "            # tar_len shape: (batch_size)\n",
    "\n",
    "            src_input = (src_ids, src_len)\n",
    "            tar_input, tar_output = slice_target(tar_ids, tar_len)\n",
    "            \n",
    "            logits = model(src_input, tar_input)\n",
    "            # logits shape: (batch_size, fixed_seq_len, tar_vocab_size)\n",
    "            \n",
    "            output = logits.argmax(2)\n",
    "            batch_idx, eos_idx =torch.nonzero(output == 2, as_tuple=True)\n",
    "            for i, b, e in zip(id_, batch_idx, eos_idx):\n",
    "                output_b = output[b,:e].tolist()\n",
    "                outputs.append(Output(1, output_b))\n",
    "                \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(src_sent, model, src_tokenizer, tar_tokenizer, device):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_ids = src_tokenizer.encode(src_sent)\n",
    "        src_len = len(src_ids)\n",
    "        \n",
    "        src_ids = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        src_len = torch.tensor([src_len], dtype=torch.long, device=device)\n",
    "        src_input = (src_ids, src_len)\n",
    "        \n",
    "        logits = model(src_input, None)\n",
    "        output = logits.argmax(1).squeeze().tolist()\n",
    "        \n",
    "        tar_sent = tar_tokenizer.decode(output)\n",
    "        \n",
    "    return tar_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, save_path, patience=5, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait agter last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improved.\n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.save_path = save_path\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        \n",
    "        score = val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            \n",
    "        elif score > self.best_score:\n",
    "            self.counter += 1 \n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        else:\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "            print(\"Saving the model to\", self.save_path)\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "- torchnlp.metrics\n",
    "    - get_moses_multi_bleu()\n",
    "    - get_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    print('Saving the model to', path)    \n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "    print('Loading the model from', path)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
