{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script - model\n",
    "## Sequence to sequence with attention\n",
    "### 참고자료\n",
    "- 논문: [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n",
    "- 코드: [IBM/pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "\n",
    "#### Model Architecture\n",
    "- encoder: stacked bidirectioanl lstm\n",
    "- decoder: stacked lstm\n",
    "    - train\n",
    "    - infer\n",
    "- attention:\n",
    "    - Dot Product Attention\n",
    "    - Multiplicative Attention\n",
    "    - Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder - Decoder\n",
    "#### notation\n",
    "- source word: \n",
    "    - $w_{1}^{(s)}, ..., w_{T_x}^{(s)}$\n",
    "- source word vector: \n",
    "    - $x_1, ..., x_{T_x}$\n",
    "- encoder hidden state: \n",
    "    - $\\overrightarrow{h_1^1}, ..., \\overrightarrow{h_{T_x}^1}$\n",
    "    - $\\overleftarrow{h_1^1}, ..., \\overleftarrow{h_{T_x}^1}$\n",
    "    - $\\overrightarrow{h_1^2}, ..., \\overrightarrow{h_{T_x}^2}$\n",
    "    - $\\overleftarrow{h_1^2}, ..., \\overleftarrow{h_{T_x}^2}$\n",
    "- attention score:\n",
    "    - $\\overrightarrow{e_1}, ..., \\overrightarrow{e_{T_x}}$\n",
    "    - $\\overleftarrow{e_1},..., \\overleftarrow{e_{T_x}}$\n",
    "- context vector:\n",
    "    - $z_{1}, ..., z_{T_y}$\n",
    "- target word:\n",
    "    - $w_{1}^{(t)}, ..., w_{T_y}^{(t)}$\n",
    "- target word vector:\n",
    "    - $y_1, ..., y_{T_y}$\n",
    "- decoder hidden state:\n",
    "    - $\\overrightarrow{s_1^1}, ..., \\overrightarrow{s_{T_y}^1}$\n",
    "    - $\\overrightarrow{s_1^2}, ..., \\overrightarrow{s_{T_y}^2}$\n",
    "- decoder output:\n",
    "    - $\\hat{y_1}, ..., \\hat{y_{\\hat{T_y}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    EncLayer (torch.nn.Module): Stacked Bidirectional LSTM Encoder Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size, embedding_dim=256, \n",
    "                 hidden_dim=256, n_layers=2, bidirectional=True, \n",
    "                 padding_idx=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding\n",
    "        self.lookup_table = nn.Embedding(src_vocab_size, embedding_dim, padding_idx)\n",
    "        \n",
    "        # Stacked Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, \n",
    "                            bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def split_states(self, states):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        states (tuple): (s, c)\n",
    "            - s \n",
    "                shape: (n_layers*n_directions, batch_size, hidden_dim)\n",
    "            - c\n",
    "                shape: (n_layers*n_directions, batch_size, hidden_dim)\n",
    "        \n",
    "        Outputs\n",
    "        -------\n",
    "        states_splitted (list of tuples): [(s_0, c_0), ..., (s_L, c_L)] *L: n_layers\n",
    "            - s_i\n",
    "                shape: (batch_size, n_directions*hidden_dim)\n",
    "            - c_i\n",
    "                shape: (batch_size, n_directions*hidden_dim)\n",
    "        \"\"\"\n",
    "        s, c = states\n",
    "        s = s.transpose(0,1).reshape(-1, self.n_layers, self.n_directions*self.hidden_dim)\n",
    "        c = c.transpose(0,1).reshape(-1, self.n_layers, self.n_directions*self.hidden_dim)\n",
    "        \n",
    "        states_splitted = [(s[:,n,:], c[:,n,:]) for n in range(self.n_layers)]\n",
    "        \n",
    "        return states_splitted\n",
    "        \n",
    "    def forward(self, src_input):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        src_input (Tuple): (src_word indices, sequence lengths) \n",
    "            - word indices (LongTensor)\n",
    "                shape: (batch_size, padded_seq_len)\n",
    "            - sequence lengths (LongTensor)\n",
    "                shape: (batch_size)\n",
    "            \n",
    "        Outputs\n",
    "        -------\n",
    "        h (PackedSequence): source hidden states\n",
    "            - data\n",
    "                shape: (packed_seq_len, n_directions*hidden_dim)\n",
    "            - lengths\n",
    "                shape: (batch_size)\n",
    "        \n",
    "        final_states (list of tuples): [(s_n_0, c_n_0), ..., (s_n_L, c_n_L)] *L: n_layers\n",
    "            - s_n_i\n",
    "                shape: (batch_size, n_directions*hidden_dim)\n",
    "            - c_n_i\n",
    "                shape: (batch_size, n_directions*hidden_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        w, seq_len = src_input\n",
    "        \n",
    "        x = self.lookup_table(w) # word vectors\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, seq_len, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        h, final_states = self.lstm(x) # encoder hidden states\n",
    "        \n",
    "        final_states = self.split_states(final_states)\n",
    "        \n",
    "        return h, final_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecCell(nn.Module):\n",
    "    \"\"\"\n",
    "    DecCell (torch.nn.Module): Stacked LSTM Decoder Cell\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, tar_vocab_size, \n",
    "                 embedding_dim=256, hidden_dim=256, n_layers=2, \n",
    "                 padding_idx=3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        n_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.enc_hidden_dim = n_directions * hidden_dim\n",
    "        self.dec_hidden_dim = self.enc_hidden_dim\n",
    "        \n",
    "        \n",
    "        # Embedding\n",
    "        self.lookup_table = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx)\n",
    "        \n",
    "        # Stacked LSTM\n",
    "        self.lstm_cells = nn.ModuleList([nn.LSTMCell(embedding_dim, self.dec_hidden_dim)])\n",
    "        for _ in range(n_layers-1):\n",
    "            self.lstm_cells.append(nn.LSTMCell(self.dec_hidden_dim, self.dec_hidden_dim))\n",
    "        \n",
    "        self.attn = attention(self.hidden_dim, bidirectional)\n",
    "        \n",
    "        # Linear Classifier\n",
    "        self.linear = nn.Linear(self.enc_hidden_dim + self.dec_hidden_dim, tar_vocab_size)\n",
    "    \n",
    "    def forward(self, w_t, h, states_t_1):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        w_t (LongTensor): target word index at timestep t\n",
    "            shape: (batch_size)\n",
    "            \n",
    "        h (PackedSequence): source hidden states\n",
    "            - data\n",
    "                shape: (packed_seq_len, n_directions*hidden_dim)\n",
    "            - lengths\n",
    "                shape: (batch_size)\n",
    "             \n",
    "        states_t_1 (list of tuples): [(s_t_1_0, c_t_1_0), ..., (s_t_1_L, c_t_1_L)] *L: n_layers\n",
    "            - s_t_1_i\n",
    "                shape: (batch_size, n_directions*hidden_dim)\n",
    "            - c_t_1_i\n",
    "                shape: (batch_size, n_directions*hidden_dim)\n",
    "            \n",
    "        Outputs\n",
    "        -------\n",
    "        logit_t (FloatTensor): target word logit at timestep t\n",
    "            shape: (batch_size, tar_vocab_size)\n",
    "             \n",
    "        states_t (list of tuples): [(s_t_1_0, c_t_1_0), ..., (s_t_1_L, c_t_1_L)] *L: n_layers\n",
    "            - s_t_i\n",
    "                shape: (batch_size, n_directions*hidden_dim)\n",
    "            - c_t_i\n",
    "                shape: (batch_size, n_directions*hidden_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        y_t = self.lookup_table(w_t)\n",
    "        # y_t shape: (batch_size, embedding_dim)\n",
    "            \n",
    "        states_t = list()\n",
    "        for n in range(self.n_layers):\n",
    "            lstm_cell_n = self.lstm_cells[n]\n",
    "            states_t_1_n = states_t_1[n]\n",
    "            input_n = y_t if n==0 else s_t\n",
    "            \n",
    "            s_t, c_t = lstm_cell_n(input_n, states_t_1_n)\n",
    "            states_t.append((s_t, c_t))\n",
    "\n",
    "        # s_t: decoder hidden state / c_t: decoder cell state\n",
    "        # s_t shape: (batch_size, hidden_dim)\n",
    "        # c_t shape: (batch_size, hidden_dim)\n",
    "\n",
    "        z_t = self.attn(s_t, h) \n",
    "        # z_t: context vector\n",
    "        # z_t shape: (batch_size, hidden_dim)\n",
    "\n",
    "        sz_t = torch.cat([s_t,z_t],1)\n",
    "        # sz_t: [s;z]\n",
    "        # sz_t shape: (batch_size, hidden_dim + hidden_dim)\n",
    "\n",
    "        logit_t = self.linear(sz_t)\n",
    "        # logits_t shape: (batch_size, tar_vocab_size)\n",
    "            \n",
    "        return logit_t, states_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecLayer(nn.Module): \n",
    "    \"\"\"\n",
    "    DecLayer (torch.nn.Module): Stacked LSTM Decoder Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, tar_vocab_size, \n",
    "                 embedding_dim=256, hidden_dim=256, n_layers=2, \n",
    "                 bidirectional=True, padding_idx=3):\n",
    "        super().__init__()\n",
    "            \n",
    "        # Stacked LSTM Cell\n",
    "        self.dec_cell = DecCell(attention, tar_vocab_size, embedding_dim, \n",
    "                                hidden_dim, n_layers, padding_idx, \n",
    "                                bidirectional)\n",
    "        \n",
    "    def _train(self, tar_ids, h, init_states):\n",
    "        \"\"\"\n",
    "        train: teacher forcing\n",
    "        \"\"\"\n",
    "        \n",
    "        w = tar_ids\n",
    "        padded_seq_len = w.shape[1]\n",
    "        # w shape: (batch_size, padded_seq_len)\n",
    "        \n",
    "        for t in range(padded_seq_len):\n",
    "                \n",
    "            # w_t, states_t_1    \n",
    "            w_t = w[:,t]\n",
    "            states_t_1 = init_states if t == 0 else states_t\n",
    "            # w_t shape: (batch_size, embedding_dim)\n",
    "            \n",
    "            # Decoder Cell\n",
    "            logit_t, states_t = self.dec_cell(w_t, h, states_t_1)\n",
    "            \n",
    "            # logits\n",
    "            logit_t = logit_t.unsqueeze(2)\n",
    "            if t == 0:\n",
    "                logits = logit_t\n",
    "            else:\n",
    "                logits = torch.cat([logits, logit_t], 2)\n",
    "            # logit_t shape: (batch_size, tar_vocab_size, 1)\n",
    "            # logits shape: (batch_size, tar_vocab_size, padded_seq_len)\n",
    "            \n",
    "        return logits\n",
    "        \n",
    "    def _infer(self, h, init_states):\n",
    "        \"\"\"\n",
    "        infer: greedy decoding\n",
    "        \"\"\"\n",
    "        # initialize w_0 with sos_id(1)\n",
    "        w_0 = torch.ones_like(h.unsorted_indices)\n",
    "        # w_0 shape: (batch_size)\n",
    "        \n",
    "        for t in range(500):\n",
    "            \n",
    "            # w_t, states_t_1\n",
    "            w_t = w_0 if t == 0 else output_t.squeeze(1)\n",
    "            states_t_1 = init_states if t==0 else states_t\n",
    "            \n",
    "            # Decoder Cell\n",
    "            logit_t, states_t = self.dec_cell(w_t, h, states_t_1)\n",
    "            \n",
    "            # logits\n",
    "            logit_t = logit_t.unsqueeze(2)\n",
    "            if t == 0:\n",
    "                logits = logit_t\n",
    "            else:\n",
    "                logits = torch.cat([logits, logit_t], 2)\n",
    "            # logit_t shape: (batch_size, 1, tar_vocab_size)\n",
    "            # logits shape: (batch_size, padded_seq_len, tar_vocab_size)\n",
    "            \n",
    "            # outputs\n",
    "            output_t = logit_t.argmax(1)\n",
    "            if t == 0:\n",
    "                outputs = output_t\n",
    "            else:\n",
    "                outputs = torch.cat([outputs, output_t], 1)\n",
    "            # output_t shape: (batch_size, 1)\n",
    "            # outputs shape: (batch_size, t+1)\n",
    "            \n",
    "            # stop iteration if all batches have eod_id(2)\n",
    "            if (outputs == 2).any(1).all():\n",
    "                break\n",
    "                \n",
    "        return logits\n",
    "        \n",
    "    def forward(self, tar_ids, h, init_states):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        tar_ids (tuple): word indices\n",
    "            - word indices (LongTensor)\n",
    "                shape: (batch_size, batch_max_seq_len)\n",
    "                \n",
    "        h (PackedSequence): source hidden states\n",
    "            - data:\n",
    "                shape: (packed_seq_len, n_directions*hidden_dim)\n",
    "            - lengths:\n",
    "                shape: (batch_size)\n",
    "        \n",
    "        init_states (tuple): (s_0, c_0)\n",
    "            - s_0 \n",
    "                shape: (batch_size, n_layers*n_directions, hidden_dim)\n",
    "            - c_0\n",
    "                shape: (batch_size, n_layers*n_directions, hidden_dim)\n",
    "            \n",
    "        Outputs\n",
    "        -------\n",
    "        logits (FloatTensor): \n",
    "            shape: (batch_size, tar_vocab_size, padded_seq_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # init logits, outputs\n",
    "        logits = None\n",
    "        outputs = None\n",
    "        if self.training:\n",
    "            logits = self._train(tar_ids, h, init_states)\n",
    "        else:\n",
    "            logits = self._infer(h, init_states)\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Attention Layer\n",
    "- BaseAttnLayer\n",
    "- DotProdAttnLayer\n",
    "- MulAttnLayer\n",
    "- AddAttnLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_directions = 2 if bidirectional == True else 1\n",
    "        \n",
    "        self.enc_hidden_dim = n_directions * hidden_dim\n",
    "        self.dec_hidden_dim = self.enc_hidden_dim\n",
    "        \n",
    "    def get_mask(self, h, seq_len):\n",
    "        \n",
    "        batch_size, padded_seq_len, hidden_dim = h.shape\n",
    "        mask = torch.zeros((batch_size, padded_seq_len), dtype=torch.bool)\n",
    "        for i in range(batch_size):\n",
    "            mask[i,seq_len[i]:] = 1\n",
    "            \n",
    "        return mask\n",
    "    \n",
    "    def compute_attn_scores(self, s_t, h):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, s_t, h):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        s_t (FloatTensor): a target hidden state at time step t\n",
    "            shape: (batch_size, hidden_dim)\n",
    "            \n",
    "        h (PackedSequence): source hidden states\n",
    "            - data:\n",
    "                shape: (packed_seq_len, n_directions*hidden_dim)\n",
    "            - lengths:\n",
    "                shape: (batch_size)\n",
    "\n",
    "        Outputs\n",
    "        -------\n",
    "        z (FloatTensor): context vector\n",
    "            shape: (batch_size, hidden_dim) or (batch_size, n_directions*hidden_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        h, seq_len = nn.utils.rnn.pad_packed_sequence(h, batch_first=True)\n",
    "        # h shape: (batch_size, padded_seq_len, n_directions*hidden_dim)\n",
    "        # seq_len: source sequence lengths\n",
    "        \n",
    "        e = self.compute_attn_scores(s_t, h) # attention scores\n",
    "        mask = self.get_mask(h, seq_len) \n",
    "        e[mask] = -float('Inf') # for masked softmax\n",
    "        # e shape: (batch_size, padded_seq_len)\n",
    "        \n",
    "        a = F.softmax(e, 1) # attention probabilities\n",
    "        # a shape: (batch_size, padded_seq_len)\n",
    "        \n",
    "        z = (a.unsqueeze(2)*h).sum(1) # context vector\n",
    "        # z shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProdAttention(BaseAttention):\n",
    "    def __init__(self, hidden_dim=256, bidirectional=True):\n",
    "        super(DotProdAttention, self).__init__(hidden_dim, bidirectional)\n",
    "        \n",
    "    def compute_attn_scores(self, s_t, h):\n",
    "        # s_t shape: (batch_size, dec_hidden_dim)\n",
    "        # h shape: (batch_size, padded_seq_len, enc_hidden_dim)\n",
    "        \n",
    "        e = (s_t.unsqueeze(1) @ h.transpose(1,2)).squeeze(2)\n",
    "        # e shape: (batch_size, padded_seq_len)\n",
    "        \n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplicative Attention\n",
    "- ${e_i} = {s_t}^{T}{W}{h_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulAttention(BaseAttention):\n",
    "    def __init__(self, hidden_dim=256, bidirectional=True):\n",
    "        super(MulAttention, self).__init__(hidden_dim, bidirectional)\n",
    "        self.W = nn.Parameter(torch.randn((self.dec_hidden_dim, self.enc_hidden_dim)*0.01))\n",
    "        \n",
    "    def compute_attn_scores(self, s_t, h):\n",
    "        # s_t shape: (batch_size, hidden_dim)\n",
    "        # h shape: (batch_size, padded_seq_len, n_directions*hidden_dim)\n",
    "        \n",
    "        e = ((s_t.unsqueeze(1) @ self.W) @ h.transpose(1,2)).squeeze(2)\n",
    "        # e shape: (batch_size, padded_seq_len)\n",
    "        \n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Attention\n",
    "- $e_i = v^{T}\\text{tanh}(W_hh_i+W_ss_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddAttention(BaseAttention):\n",
    "    def __init__(self, hidden_dim=256, bidirectional=True):\n",
    "        super(AddAttention, self).__init__(hidden_dim, bidirectional)\n",
    "        \n",
    "        self.W_s = nn.Parameter(torch.randn((self.dec_hidden_dim, self.dec_hidden_dim), dtype=torch.float)*0.01)\n",
    "        self.W_h = nn.Parameter(torch.randn((self.enc_hidden_dim, self.dec_hidden_dim), dtype=torch.float)*0.01)\n",
    "        self.v = nn.Parameter(torch.randn((self.dec_hidden_dim,1), dtype=torch.float)*0.01)\n",
    "        \n",
    "    def compute_attn_scores(self, s_t, h):\n",
    "        # s_t shape: (batch_size, dec_hidden_dim)\n",
    "        # h shape: (batch_size, padded_seq_len, enc_hidden_dim)\n",
    "        \n",
    "        e = (torch.tanh((s_t.unsqueeze(1) @ self.W_s) + (h @ self.W_h)) @ self.v).squeeze(2)\n",
    "        # e shape: (batch_size, padded_seq_len)\n",
    "        \n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Seq2SeqWithAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    def __init__(self, src_vocab_size, tar_vocab_size, \n",
    "                 attention, embedding_dim=256, hidden_dim=256, \n",
    "                 n_layers=2, bidirectional=True):\n",
    "        \n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tar_vocab_size = tar_vocab_size\n",
    "        \n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttn(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = EncLayer(config.src_vocab_size, config.embedding_dim, config.hidden_dim, \n",
    "                                config.n_layers, config.bidirectional)\n",
    "        \n",
    "        self.decoder = DecLayer(config.attention, config.tar_vocab_size, config.embedding_dim, \n",
    "                                config.hidden_dim, config.n_layers, config.bidirectional)\n",
    "        \n",
    "    def forward(self, src_input, tar_ids):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        src_input (Tuple): (src_word indices, sequence lengths) \n",
    "            - word indices (LongTensor)\n",
    "                shape: (batch_size, padded_seq_len)\n",
    "            - sequence lengths (LongTensor)\n",
    "                shape: (batch_size)\n",
    "                \n",
    "        tar_ids (tuple): word indices \n",
    "            - word indices (LongTensor)\n",
    "                shape: (batch_size, batch_max_seq_len)\n",
    "\n",
    "        Outputs\n",
    "        -------\n",
    "        logits (FloatTensor): \n",
    "            shape: (batch_size, padded_seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        h, enc_final_states = self.encoder(src_input)\n",
    "        \n",
    "        dec_init_states = enc_final_states\n",
    "        logits = self.decoder(tar_ids, h, dec_init_states)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configuration(src_vocab_size=32000, tar_vocab_size=32000, attention=AddAttention)\n",
    "model = Seq2SeqWithAttn(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqWithAttn(\n",
       "  (encoder): EncLayer(\n",
       "    (lookup_table): Embedding(32000, 256, padding_idx=3)\n",
       "    (lstm): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): DecLayer(\n",
       "    (dec_cell): DecCell(\n",
       "      (lookup_table): Embedding(32000, 256, padding_idx=3)\n",
       "      (lstm_cells): ModuleList(\n",
       "        (0): LSTMCell(256, 512)\n",
       "        (1): LSTMCell(512, 512)\n",
       "      )\n",
       "      (attn): AddAttention()\n",
       "      (linear): Linear(in_features=1024, out_features=32000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56016640"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
