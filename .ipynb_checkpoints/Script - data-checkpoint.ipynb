{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script - data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 출처: [The Stanford Natural Language Processing Group](https://nlp.stanford.edu/projects/nmt/)\n",
    "- Train set\n",
    "    - [train.en](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en)\n",
    "    - [train.de](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de)\n",
    "- Test set\n",
    "    - [newstest2013.en](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2013.en)\n",
    "    - [newstest2013.de](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2013.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download\n",
    "#### Train: WMT 2014 en-de\n",
    "- WMT'14 EN\n",
    "    - size: 0.65GB\n",
    "    - \\# of sents: 4.5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-27 17:34:51--  https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 644937323 (615M) [text/plain]\n",
      "Saving to: ‘./data/train.en’\n",
      "\n",
      "train.en            100%[===================>] 615.06M  1.92MB/s    in 9m 56s  \n",
      "\n",
      "2019-08-27 17:44:47 (1.03 MB/s) - ‘./data/train.en’ saved [644937323/644937323]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en -P ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WMT'14 DE\n",
    "    - size: 0.72GB\n",
    "    - \\# of sents: 4.5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-27 17:44:47--  https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 717610118 (684M) [text/plain]\n",
      "Saving to: ‘./data/train.de’\n",
      "\n",
      "train.de            100%[===================>] 684.37M  3.02MB/s    in 7m 25s  \n",
      "\n",
      "2019-08-27 17:52:13 (1.54 MB/s) - ‘./data/train.de’ saved [717610118/717610118]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de -P ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: newstest2013\n",
    "- newstest13 EN\n",
    "    - size: 356KB\n",
    "    - \\# of sents: 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-27 17:52:13--  https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2013.en\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 355027 (347K) [text/plain]\n",
      "Saving to: ‘./data/newstest2013.en’\n",
      "\n",
      "newstest2013.en     100%[===================>] 346.71K   350KB/s    in 1.0s    \n",
      "\n",
      "2019-08-27 17:52:15 (350 KB/s) - ‘./data/newstest2013.en’ saved [355027/355027]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2013.en -P ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- newstest13 DE\n",
    "    - size: 410KB\n",
    "    - \\# of sents: 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-27 17:52:15--  https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2013.de\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 405428 (396K) [text/plain]\n",
      "Saving to: ‘./data/newstest2013.de’\n",
      "\n",
      "newstest2013.de     100%[===================>] 395.93K   387KB/s    in 1.0s    \n",
      "\n",
      "2019-08-27 17:52:17 (387 KB/s) - ‘./data/newstest2013.de’ saved [405428/405428]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2013.de -P ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "### Build Vocabulary: SentencePiece\n",
    "- vocab_size: 32000(32K)\n",
    "\n",
    "#### WPM_EN\n",
    "- model type: unigram(WPM)\n",
    "- outputs\n",
    "    - WPM_EN.model\n",
    "    - WPM_EN.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 53s, sys: 4.26 s, total: 8min 57s\n",
      "Wall time: 5min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spm.SentencePieceTrainer.Train('--input=./data/train.en \\\n",
    "--model_prefix=WPM_EN \\\n",
    "--vocab_size=32000 \\\n",
    "--character_coverage=1.0 \\\n",
    "--model_type=unigram \\\n",
    "--pad_id=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WPM_DE\n",
    "- model type: WPM\n",
    "- outputs\n",
    "    - WPM_DE.model\n",
    "    - WPM_DE.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 29s, sys: 3.41 s, total: 16min 33s\n",
      "Wall time: 6min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spm.SentencePieceTrainer.Train('--input=./data/train.de \\\n",
    "--model_prefix=WPM_DE \\\n",
    "--vocab_size=32000 \\\n",
    "--character_coverage=1.0 \\\n",
    "--model_type=unigram \\\n",
    "--pad_id=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE_EN\n",
    "- model type: BPE\n",
    "- outputs\n",
    "    - BPE_EN.model\n",
    "    - BPE_EN.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 1.75 s, total: 2min 45s\n",
      "Wall time: 2min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spm.SentencePieceTrainer.Train('--input=./data/train.en \\\n",
    "--model_prefix=BPE_EN \\\n",
    "--vocab_size=32000 \\\n",
    "--character_coverage=1.0 \\\n",
    "--model_type=bpe \\\n",
    "--pad_id=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE_DE\n",
    "- model type: BPE\n",
    "- outputs\n",
    "    - BPE_DE.model\n",
    "    - BPE_DE.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 38s, sys: 2.28 s, total: 4min 40s\n",
      "Wall time: 4min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spm.SentencePieceTrainer.Train('--input=./data/train.de \\\n",
    "--model_prefix=BPE_DE \\\n",
    "--vocab_size=32000 \\\n",
    "--character_coverage=1.0 \\\n",
    "--model_type=bpe \\\n",
    "--pad_id=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = spm.SentencePieceProcessor()\n",
    "        self.tokenizer.load(model_path)\n",
    "        \n",
    "    def encode(self, sentence):\n",
    "        \"\"\" encode: a sentence to list of ids \"\"\"\n",
    "        \n",
    "        return self.tokenizer.encode_as_ids(sentence)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\" decode: list of ids to a sentence \"\"\"\n",
    "        \n",
    "        return self.tokenizer.DecodeIds(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sent_pairs, src_tokenizer, tar_tokenizer, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tar_tokenizer = tar_tokenizer\n",
    "        self.device = device\n",
    "        self.sent_pairs = sent_pairs # list of tuples\n",
    "        \n",
    "    @classmethod    \n",
    "    def from_txt(cls, src_path, tar_path, src_tokenizer, tar_tokenizer, device):\n",
    "        \n",
    "        SentPair = namedtuple('SentPair', ['id','src_sent', 'tar_sent'])\n",
    "        sent_pairs = list()\n",
    "        with open(src_path, 'r') as src_file, open(tar_path, 'r') as tar_file:\n",
    "            for id_, (src_sent, tar_sent) in enumerate(zip(src_file.readlines(), tar_file.readlines())):\n",
    "                sent_pair = SentPair(id_, src_sent, tar_sent)\n",
    "                sent_pairs.append(sent_pair)\n",
    "                \n",
    "        return cls(sent_pairs, src_tokenizer, tar_tokenizer, device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.sent_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.sent_pairs[idx]\n",
    "    \n",
    "    def _preprocess(self, sent_pair):\n",
    "        \"\"\"sentence to index, length\"\"\"\n",
    "        \n",
    "        id_, src_sent, tar_sent = sent_pair\n",
    "        src_ids = self.src_tokenizer.encode(src_sent)\n",
    "        tar_ids = [1]+self.tar_tokenizer.encode(tar_sent)+[2]\n",
    "        # bos_id: 1 / eos_id: 2\n",
    "        src_len = len(src_ids)\n",
    "        tar_len = len(tar_ids)\n",
    "        \n",
    "        return id_, src_ids, src_len, tar_ids, tar_len\n",
    "    \n",
    "    def _collate(self, batch):\n",
    "        \"\"\"list of index, length to tensor\"\"\"\n",
    "        \n",
    "        id_list = list()\n",
    "        src_ids_list = list()\n",
    "        src_len_list = list()\n",
    "        tar_ids_list = list()\n",
    "        tar_len_list = list()\n",
    "        \n",
    "        for sent_pair in batch:\n",
    "            id_, src_ids, src_len, tar_ids, tar_len = self._preprocess(sent_pair)\n",
    "            id_list.append(id_)\n",
    "            src_ids_list.append(torch.tensor(src_ids, dtype=torch.long, device=self.device)) \n",
    "            tar_ids_list.append(torch.tensor(tar_ids, dtype=torch.long, device=self.device)) \n",
    "            src_len_list.append(src_len)\n",
    "            tar_len_list.append(tar_len)\n",
    "        \n",
    "        id_ = id_list\n",
    "        src_ids = nn.utils.rnn.pad_sequence(src_ids_list, batch_first=True, padding_value=3)\n",
    "        tar_ids = nn.utils.rnn.pad_sequence(tar_ids_list, batch_first=True, padding_value=3)\n",
    "        \n",
    "        src_len = torch.tensor(src_len_list, dtype=torch.long, device=self.device)\n",
    "        tar_len = torch.tensor(tar_len_list, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        return id_, src_ids, src_len, tar_ids, tar_len\n",
    "    \n",
    "    def _split(self, dataset):\n",
    "        \"\"\"split train/test set \"\"\"\n",
    "    \n",
    "        train_size = int(0.7 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "        \n",
    "    def to_dataloader(self, batch_size=128, n_workers=0, split=True):\n",
    "        res = None \n",
    "        if split:\n",
    "            train_dataset, test_dataset = self._split(self)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=self._collate, \n",
    "                                          num_workers=n_workers)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=self._collate, \n",
    "                                         num_workers=n_workers)\n",
    "            \n",
    "            res = train_dataloader, test_dataloader\n",
    "        else:\n",
    "            dataloader = DataLoader(self, batch_size=batch_size, collate_fn=self._collate, num_workers=n_workers)\n",
    "            \n",
    "            res = dataloader\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.18 s, sys: 898 ms, total: 9.08 s\n",
      "Wall time: 9.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "src_tokenizer = Tokenizer('BPE_EN.model')\n",
    "tar_tokenizer = Tokenizer('BPE_DE.model')\n",
    "dataset = NMTDataset.from_txt('./data/train.en', './data/train.de', src_tokenizer, tar_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 217 ms, total: 1.52 s\n",
      "Wall time: 915 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "train_dataloader, valid_dataloader = dataset.to_dataloader(batch_size=8, n_workers=0, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch(dataloader):\n",
    "    batch = next(iter(dataloader))\n",
    "    for i in range(len(batch)):\n",
    "        if i == 0:\n",
    "            print(len(batch[i]))\n",
    "        else:\n",
    "            print(batch[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([8, 41])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 40])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "check_batch(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([8, 58])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 59])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "check_batch(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 97 µs, sys: 10 µs, total: 107 µs\n",
      "Wall time: 118 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "dataloader = dataset.to_dataloader(batch_size=8, n_workers=0, split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([8, 35])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 54])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "check_batch(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
